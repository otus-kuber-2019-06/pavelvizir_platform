git checkout -b kubernetes-networks
# Plan:
# test web-app
#  pod healthchecks
#  deployments
#  clusterip
#  ipvs
# ingress access
#  metallb l2
#  loadbalancer
#  ingress-controller
#  ingress rules

# add healthchecks
vim kubernetes-intro/web-pod.yaml
...
readinessProbe:
  httpGet:
    path: /index.html
    port: 80
...

kubectl delete pods web
kubectl apply -f kubernetes-intro/web-pod.yaml
kubectl get pods # make sure it's in Running state
kubectl describe pod/web # look for conditions and events

# add liveness probe despite wrong port
vim kubernetes-intro/web-pod.yaml
...
livenessProbe:
  tcpSocket:
    port: 8000

kubectl delete pods web --force --grace-period=0
kubectl apply -f kubernetes-intro/web-pod.yaml

# questions:
1. what's wrong with the following?
```sh
livenessProbe:
  exec:
command:
      - 'sh'
      - '-c'
      - 'ps aux | grep my_web_server_process'
```
 - checking for main process health is dumb. Make it exit in case of errors.
2. when may you use that probe?
 - when you've got multiple processes in pod i suppose

# let's go further
# deployment
mkdir kubernetes-networks
cd kubernetes-networks
cp ../kubernetes-intro/web-pod.yaml web-deploy.yaml
vim web-deploy.yaml
... pod -> deploy, v1 -> apps/v1, spec shift with 2 positions

kubectl apply -f web-deploy.yaml
kubectl describe deployment.apps web

# time to fix error in probe, increase replicas to 3
kubectl apply -f web-deploy.yaml

# next task
# check conditions status equals true
kubectl describe deploymenta.apps web
# add strategy block
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 0
    maxSurge: 100%

# now play with values ot Surge and Unavailable
# monitor process with kubectl get events --watch / kubespy trace deploy web
# `brew install kubespy`
# kubespy uses deprecated api :-(

# intermediate commit

# now to service creation
cat web-svc-cip.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: web-svc-cip
spec:
  selector:
    app: web
  type: ClusterIP
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000

kubectl apply -f web-svc-cip.yaml

# let's test something
minikube ssh
sudo -i
curl http://10.97.84.61/index.html # OK
ping 10.97.84.61 # FAIL
ip a | grep '10.97.84.61' # FAIL
iptables -nv -t nat -L | grep -q '10.97.84.61' && echo OK

# let's switch to IPVS

kubectl --namespace kube-system edit configmaps kube-proxy
mode: "" -> mode: "ipvs"
# now delete pod to restart it with new settings
kubectl --namespace kube-system delete pod --selector='k8s-app=kube-proxy'

# recheck everything
minikube ssh
sudo -i 
iptables -nv -t nat -L # old chains are here
# kube-proxy --cleanup doesn't work
# kubectl --namespace kube-system exec kube-proxy-<POD> kube-proxy --cleanup

echo -e '*nat
-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE
COMMIT
*filter
COMMIT
*mangle
COMMIT
' >> /tmp/iptables.cleanup
iptables-restore < /tmp/iptables.cleanup
# check again or wait and check again :-\
iptables -nv -t nat -L
# to have ipvsadm `toolbox` - to obtain fedore o.O
toolbox
dnf install -y ipvsadm && dnf clean all
ipvsadm --list -n
# then exit toolbox and ping cluster ip 
ip a kube-ipvs0 # there is real interface
# ipset allows to see ipsets :-)
